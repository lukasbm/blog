+++
title = "The Ultimate Setup for AI Research"
author = "Lukas BÃ¶hm"
description = "Using tools from MLOps to accelerate your AI research"
+++

Many researches are often presented with having to evaluate a new model on
multiple datasets or sets of hyperparameters.
Research code is famously known for having awful quality due to it's nature of being thrown away after the publication.
Often times no reproducible environment is set up and everything is crammed into a hand-full of scripts.
These overloaded scripts often try do everything from data manipulation, training and evaluation at once,
but more often just ends up as a giant pile of spagetti nobody wants to deal with.
By the time you write your paper and create the graphs and figures your entire workflow
has become so messy that your productivity grinds to a painful halt.

If you want to pride yourself as a good researcher that publishes high quality, reproducible code,
you might want to stick around as we explore how we can employ two technologies
to completely transform your workflow. 

## Problems and their causes

TODO: mention slurm is often used

## Solution

## Glimpse into the Future

# Comments
<!-- 
<script src="https://giscus.app/client.js"
        data-repo="lukasbm/blog"
        data-repo-id="R_kgDOLBREVQ"
        data-category="General"
        data-category-id="DIC_kwDOLBREVc4CcOfk"
        data-mapping="title"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script> -->
